
基础路径设置:
basePath = Save:node\Pythons
dataPath = D:\project\S_name\3^sql^data


/***********************************************************************/

// 1. Pythons 中文
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.1\] BISIC
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.2\] RNN 没有并行化
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.3\] LSTM
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.4\] biLM 双向LSTM
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.5\] ELMo 训练语言模型
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.6\] BERT 新语言表示模型
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.7\] ESIM
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.8\] Transformer 提取特征
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.9\] GPT 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.10\] CNN 卷积 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.11\] 云计算
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.12\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.13\]
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.14\]
// 2. lib
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.1\] keras
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.2\] Keras Transformer------test
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.3\] Activation-------------激活函数
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.4\] 指标
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.5\] NLP 任务
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.6\] keras.Lambda
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.7\] 分模块
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.8\] load_model--h5
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.9\] Keras 中文
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.10\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.11\] keras weight----------可训练的/不可训练
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.12\] GradientTape----------修改梯度
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.13\] tf.function-----------静态图
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.14\] 不稳定层
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.15\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.16\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.17\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.18\] 
// 3. project
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.1\] Demo ------------------(无用的demo)
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.2\] Demo ------------------(有用的demo)
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.3\] Demo ------------------临时
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.4\] tf1_bert
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.5\] tf2_bert
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.6\] game
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.7\] 二分类-----------------分词++
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.8\] layers-----------------简单层例子
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.9\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.10\] 



[1.1] BISIC

Blockchain/AI/Security/IoT/Computer
块链/ AI /安全性/物联网/计算机




[1.2] RNN
RNN（Recurrent neural networks）循环神经网络





[1.3] LSTM
LSTM (Long Short Term Memory)




[1.4] BiLM 双向LSTM
双向语言模型（BiLM）Bi-LSTM




[1.5] ELMo 训练语言模型
ELMO 是“Embedding from Language Models”的简称
ELMo最重要的一点是就是训练语言模型




[1.6] BERT 新语言表示模型
(Bidirectional Encoder Representations from Transformers)




[1.7] ESIM




[1.8] Transformer 提取特征
Transformer 是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前 NLP 里最强的特征提取器

深度学习中的注意力模型




[1.9] GPT 
GPT 基于 Fine-tuning 的模式




[1.10] CNN
是其先天的卷积操作不很适合序列化的文本



[1.11] 云计算
//ngc.nvidia.com
//免费机时可能用


[1.12] 




[1.13] 
	

[1.14] 


[1.15] 



[1.16] 



[2.1] keras
// keras_docs
py_tf2\keras_docs.py
python_w py_tf2\keras_docs.py

// keras_softmax
py_tf2\keras_softmax.py
python_w py_tf2\keras_softmax.py
//	name 'keras' is not defined

// keras_MLP
py_tf2\keras_MLP.py
python_w py_tf2\keras_MLP.py

// keras_VGG
py_tf2\keras_VGG.py
python_w py_tf2\keras_VGG.py

// keras_LSTM
py_tf2\keras_LSTM.py
python_w py_tf2\keras_LSTM.py
//	name 'max_features' is not defined

// keras_Conv1D--------1D卷积
py_tf2\keras_Conv1D.py
python_w py_tf2\keras_Conv1D.py
//	name 'seq_length' is not defined

// keras_Sequential--------用于序列分类的栈式LSTM
py_tf2\keras_Sequential.py
python_w py_tf2\keras_Sequential.py

//keras_stateful--------采用stateful LSTM的相同模型
py_tf2\keras_stateful.py
python_w py_tf2\keras_stateful.py



[2.2] Keras Transformer
//train
py_tf2\transformer_train.py
python_w py_tf2\transformer_train.py


//predict
py_tf2\transformer_predict.py
python_w py_tf2\transformer_predict.py


//translation
py_tf2\transformer_translation.py
python_w py_tf2\transformer_translation.py


//beam_Search
py_tf2\transformer_beam_Search.py
python_w py_tf2\transformer_beam_Search.py



[2.3] Activation---激活函数
softmax:           [0__1]                         多分类, 计算量大
elu:               [-1__-0.8__0__10]
selu:              [-1__-1__0__10]
softplus:          [0__0.6__10]
softsign:          [-1__-0.6__0__0.6__1]
relu:              [0__0__10]
//LReLU:           [-2__0__10]
//PReLU:           [-1__0__10]
//CReLU:           [, ]
tanh:              [-1__-0.7__0__0.7__1]
sigmoid:           [-1__-0.7__0__0.7__1]          二分类, 计算量相对大;梯度消失
hard_sigmoid:      [, ]
linear:            [0__0__1__1]
//LeakyReLU:
//ParametricSoftplus:
//ThresholdedReLU: [0__0__0__10]


[2.4] 指标
// 指标
//	精准率: precision rate = TP / (TP + FP)
//	召回率: recall rate = TP / (TP + FN)
//	准确率: accuracy = (TP + TN) / (TP + FP + TN + FN)
//	F1-score = 2 * precision rate * recall rate / (precision rate + recall rate)
//	F1-score = 2 * 精 * 召 / (精 + 召)

//	精准率: __FP TP__>>0000 1111, 推荐系统-----重心--正样>负样---调多=调少
//	精准率: __11 11__>>0000 1111, 
//	召回率: ____ TPFN>>0000 1111, 逃犯检索-----直径--仅正样影响
//	召回率: ____ 1100>>0000 1111,
//	准确率: TNFP TPFN>>0000 1111, XXXX系统-----边界--正样=负样---调多>调少
//	准确率: 0011 1100>>0000 1111,
//	F1-scr: __11 1100>>0000 1111, XXXX系统-----XX界--正样>负样---正多>正少---负少>正多
//真阳性率: ____ TPFN>>0000 1111, TPR=召回率
//假阴性率: TNFP ____>>0000 1111, FPR




[2.5] NLP 任务
// NLP_task 任务
Save:node\Pythons\project\Macro_Pythons_explain_nlp.h  NLP_task
// NLP_lib 6个顶级Python NLP库的比较
Save:node\Pythons\project\Macro_Pythons_explain_nlp.h  NLP_lib
// NLP_fc 六款中文分词模块
Save:node\Pythons\project\Macro_Pythons_explain_nlp.h  NLP_fc


[2.6] keras.Lambda
//Lambda--Sequential
py_tf2\Keras_ly_Lambda.py
python_w py_tf2\Keras_ly_Lambda.py

//mylayer--Sequential
py_tf2\Keras_ly_mylayer.py
python_w py_tf2\Keras_ly_mylayer.py


//Lambda--多输入
py_tf2\Keras_ly_input.py
python_w py_tf2\Keras_ly_input.py


[2.7] 分模块
// loss参考
https://github.com/kundajelab/keras-genomics/blob/master/keras_genomics/losses.py


[2.8] load_model--h5
//在Keras中，如果存在自定义layer或者loss，需要在load_model()中以字典形式指定layer或loss。
model = load_model(‘My_model.h5’, custom_objects={‘NEW_loss’:NEW_loss,‘NEW_LAYER’: NEW_LAYER})


[2.9] Keras 中文

// Keras 中文
py_tf2\Keras_text_cn.py
python_w py_tf2\Keras_text_cn.py



[2.10] 



[2.11] keras layer-----------可训练的/不可训练
由 layers 创建的权重可以是可训练的，也可以是不可训练的。
是否可训练可以在 layer 的属性 “trainable_weights” 和 “non_trainable_weights” 中看到。
比如，这是一个具有不可训练权重的层：


[2.12] GradientTape----------修改梯度
你可以通过在 GradientTape 中调用 layer 来自动检索该层权重的梯度。
使用这些梯度，你可以手动或使用优化器对象来更新 layer 的权重。
当然，你也可以在使用梯度之前修改它们。


[2.13] tf.function-----------静态图


[2.14] 不稳定层
有些层，特别是 “BatchNormalization” 层和 “退 Dropout” 层，在训练和推理过程中会表现出不同的行为。
对于这样的层，标准做法是在 “call” 方法中加入一个 “training”(boolean) 参数。


[2.15] 


[2.16] 


[2.17] 


[2.18] 


[2.19] 


[2.20] 


[2.21] 


[2.22] 


[2.23] 


[2.24] 


[2.25] 


[2.26] 
	






[3.1] Demo ------------------(无用的demo)
//Demo
// 情感分析（sentiment analysis）
https://blog.csdn.net/weixin_34351321/article/details/89627713


// Keras文本标签2(要下载数据, 很大)
py_tf2\Keras_text_type.py
python_w py_tf2\Keras_text_type.py


// BERT文本分类2(tf?)
https://github.com/yongzhuo/Keras-TextClassification


// BERT文本分类3(tf1.9)
//https://blog.csdn.net/rensihui/article/details/90648291
https://github.com/yongzhuo/nlp_xiaojiang
// project + setup.py:
project\Macro_nlp_xiaojiang.h


// Transfromer模型
//   Transfromer模型代码实现（基于Keras）
https://github.com/xiaosongshine/transfromer_keras
// project:
project\Macro_nlp_transfromer_keras.h





[3.2] Demo ------------------(有用的demo)
//tf2_Demo---------------的样例
//tf2_Examples
project\Macro_api_demo.h

tf2_chinese
// BERT 的全称是基于 Transformer 的双向编码器表征
//     log\Log_...
//     ipynb转py
// project:
project\Macro_nlp_tf2_chinese.h




[3.3] Demo ------------------临时
demoPath = D:\project\Demo

// 评估--cb_metrics------f1_score, 图也出不来
demo:tf2_tensorboard\example_cb_metrics.py
python_w demo:tf2_tensorboard\example_cb_metrics.py



[3.4] tf1_bert

// bert_google:
https://github.com/google-research/bert
project\Macro_nlp_bert_google.h

// keras-bert:----可以参数导入普通文本训练词间关系
//https://github.com/strongio/keras-bert

// BERT_chinese:
https://github.com/mastercaojie/BERT-chinese
project\Macro_nlp_BERT_chinese.h



[3.5] tf2_bert
//search:BERT_tf

// project1:
https://github.com/kpe/bert-for-tf2
project\Macro_nlp_bert_for_tf.h


// project2:
https://github.com/strongio/keras-bert
project\Macro_nlp_keras_bert.h




[3.6] game
	
//game_nlp
Save:node\Pythons\project\Macro_Pythons_game_nlp.h

//中国法研杯



[3.7] 二分类-----------------分词++
//二分类--分词++
https://github.com/elephantnose/Text-classification
https://www.jianshu.com/p/e091dba8eaaf
project\Macro_nlp_classier.h


//二分类
categorical_crossentropy
keras.utils.to_categorical，即便num_classes=2


[3.8] layers

// layers_SCALER
//   训练系数，使输出是输入的0.1
py_test\tf2_layers_SCALER.py
python_w py_test\tf2_layers_SCALER.py



[3.9] 



[3.10] 






