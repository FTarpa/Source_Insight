
基础路径设置:
basePath = Save:node\Pythons
dataPath = D:\project\S_name\3^sql^data


/***********************************************************************/

// 1. Pythons 练习
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.1\] BISIC
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.2\] RNN 没有并行化
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.3\] LSTM
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.4\] biLM 双向LSTM
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.5\] ELMo 训练语言模型
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.6\] BERT 新语言表示模型
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.7\] ESIM
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.8\] Transformer 提取特征
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.9\] GPT 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.10\] CNN 卷积 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.11\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.12\]
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.13\]
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.14\]
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.15\]
Save:node\Pythons\study\Macro_Pythons_tf2.h \[1.16\]
// 2. lib
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.1\] keras
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.2\] Keras Transformer
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.3\] Activation---激活函数
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.4\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.5\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.6\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[2.10\] NLP 任务
// 3. Demo
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.1\] tf2_chinese
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.2\] Transfromer模型
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.3\] BERT文本分类(tf1.9)
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.4\] Keras情感分析
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.5\] Keras文本标签(数据下载不了)
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.6\] Keras实现注意力(代码不全)
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.7\] BERT文本分类2(tf?)
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.8\] search:BERT_tf----------------------
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.9\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[3.10\] 
// 4. game
Save:node\Pythons\study\Macro_Pythons_tf2.h \[4.1\] game_nlp
Save:node\Pythons\study\Macro_Pythons_tf2.h \[4.2\] 中国法研杯
Save:node\Pythons\study\Macro_Pythons_tf2.h \[4.3\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[4.4\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[4.5\] 
// 5. tf1.0
Save:node\Pythons\study\Macro_Pythons_tf2.h \[5.1\] atec_2018_nlp
Save:node\Pythons\study\Macro_Pythons_tf2.h \[5.2\] Financial_NLP
Save:node\Pythons\study\Macro_Pythons_tf2.h \[5.3\] bert_google
Save:node\Pythons\study\Macro_Pythons_tf2.h \[5.4\] BERT_chinese
Save:node\Pythons\study\Macro_Pythons_tf2.h \[5.5\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[5.6\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[5.7\] 
Save:node\Pythons\study\Macro_Pythons_tf2.h \[5.8\] 
// 其他标号
Save:SI\node\Test\Macro_Tmp_Node_Num.h \[1.1\] 



[1.1] BISIC

Blockchain/AI/Security/IoT/Computer
块链/ AI /安全性/物联网/计算机




[1.2] RNN
RNN（Recurrent neural networks）循环神经网络





[1.3] LSTM
LSTM (Long Short Term Memory)




[1.4] BiLM 双向LSTM
双向语言模型（BiLM）Bi-LSTM




[1.5] ELMo 训练语言模型
ELMO 是“Embedding from Language Models”的简称
ELMo最重要的一点是就是训练语言模型




[1.6] BERT 新语言表示模型
(Bidirectional Encoder Representations from Transformers)




[1.7] ESIM




[1.8] Transformer 提取特征
Transformer 是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前 NLP 里最强的特征提取器

深度学习中的注意力模型




[1.9] GPT 
GPT 基于 Fine-tuning 的模式




[1.10] CNN
是其先天的卷积操作不很适合序列化的文本


[1.11] 


[1.12] 



[1.13] 
	

[1.14] 


[1.15] 



[1.16] 



[2.1] keras
//keras_softmax
py_tf2\keras_softmax.py
python_w py_tf2\keras_softmax.py
//	name 'keras' is not defined


//keras_MLP
py_tf2\keras_MLP.py
python_w py_tf2\keras_MLP.py
//	Epoch 1/20
//	1000/1000 [==============================] - 2s 2ms/step - loss: 0.7084 - accuracy: 0.5130
//	Epoch 2/20
//	1000/1000 [==============================] - 0s 31us/step - loss: 0.7125 - accuracy: 0.5110


//keras_VGG
py_tf2\keras_VGG.py
python_w py_tf2\keras_VGG.py
//	Epoch 1/10
//	100/100 [==============================] - 6s 56ms/step - loss: 2.3383
//	Epoch 2/10
//	100/100 [==============================] - 5s 47ms/step - loss: 2.2947


//keras_LSTM
py_tf2\keras_LSTM.py
python_w py_tf2\keras_LSTM.py
//	name 'max_features' is not defined


//keras_Conv1D--------1D卷积
py_tf2\keras_Conv1D.py
python_w py_tf2\keras_Conv1D.py
//	name 'seq_length' is not defined


//keras_Sequential--------用于序列分类的栈式LSTM
py_tf2\keras_Sequential.py
python_w py_tf2\keras_Sequential.py
//	Train on 1000 samples, validate on 100 samples
//	Epoch 1/5
//	1000/1000 [==============================] - 2s 2ms/step - loss: 11.9961 - accuracy: 0.1230 - val_loss: 12.3788 - val_accuracy: 0.1300
//	Epoch 2/5
//	1000/1000 [==============================] - 0s 202us/step - loss: 12.7283 - accuracy: 0.1280 - val_loss: 12.5455 - val_accuracy: 0.1300


//keras_stateful--------采用stateful LSTM的相同模型
py_tf2\keras_stateful.py
python_w py_tf2\keras_stateful.py
//	Train on 320 samples, validate on 96 samples
//	Epoch 1/5
//	320/320 [==============================] - 1s 4ms/step - loss: 11.5509 - accuracy: 0.0938 - val_loss: 11.7102 - val_accuracy: 0.0938
//	Epoch 2/5
//	320/320 [==============================] - 0s 390us/step - loss: 11.7549 - accuracy: 0.1125 - val_loss: 11.8945 - val_accuracy: 0.1042




[2.2] Keras Transformer
//train
py_tf2\transformer_train.py
python_w py_tf2\transformer_train.py


//predict
py_tf2\transformer_predict.py
python_w py_tf2\transformer_predict.py


//translation
py_tf2\transformer_translation.py
python_w py_tf2\transformer_translation.py


//beam_Search
py_tf2\transformer_beam_Search.py
python_w py_tf2\transformer_beam_Search.py



[2.3] Activation---激活函数
softmax:           [0__1]                         多分类, 计算量大
elu:               [-1__-0.8__0__10]
selu:              [-1__-1__0__10]
softplus:          [0__0.6__10]
softsign:          [-1__-0.6__0__0.6__1]
relu:              [0__0__10]
//LReLU:           [-2__0__10]
//PReLU:           [-1__0__10]
//CReLU:           [, ]
tanh:              [-1__-0.7__0__0.7__1]
sigmoid:           [-1__-0.7__0__0.7__1]          二分类, 计算量相对大;梯度消失
hard_sigmoid:      [, ]
linear:            [0__0__1__1]
//LeakyReLU:
//ParametricSoftplus:
//ThresholdedReLU: [0__0__0__10]


[2.4] 


[2.5] 


[2.6] 


[2.7] 


[2.8] 


[2.9] 


[2.10] NLP 任务
// NLP_task 任务
Save:node\Pythons\project\Macro_Pythons_explain_nlp.h  NLP_task
// NLP_lib 6个顶级Python NLP库的比较
Save:node\Pythons\project\Macro_Pythons_explain_nlp.h  NLP_lib
// NLP_fc 六款中文分词模块
Save:node\Pythons\project\Macro_Pythons_explain_nlp.h  NLP_fc








[3.1] tf2_chinese
// BERT 的全称是基于 Transformer 的双向编码器表征
//     log\Log_...
//     ipynb转py
// project:
project\Macro_nlp_tf2_chinese.h



[3.2] Transfromer模型
//  Transfromer模型代码实现（基于Keras）
//https://github.com/xiaosongshine/transfromer_keras
// project:
project\Macro_nlp_transfromer_keras.h




[3.3] BERT文本分类(tf1.9)
//BERT文本分类
https://blog.csdn.net/rensihui/article/details/90648291
// project:
project\Macro_nlp_xiaojiang.h




[3.4] Keras情感分析
//情感分析（sentiment analysis）
https://blog.csdn.net/weixin_34351321/article/details/89627713



[3.5] Keras文本标签(数据下载不了)
// Keras进行单标签多文本
https://blog.csdn.net/qiqi123i/article/details/98473234


// (要下载数据, 很大)
//text_type
py_tf2\Keras_text_type.py
python_w py_tf2\Keras_text_type.py



[3.6] Keras实现注意力(代码不全)
// (代码不全)
// Keras实现注意力机制
//https://blog.csdn.net/uhauha2929/article/details/80733255



[3.7] BERT文本分类2(tf?)
https://github.com/yongzhuo/Keras-TextClassification




[3.8] search:BERT_tf
// project1:
https://github.com/kpe/bert-for-tf2
project\Macro_nlp_bert_tf2_1.h


// project2:
https://github.com/strongio/keras-bert
project\Macro_nlp_bert_tf2_2.h




[3.9] 



[3.10] 



[4.1] game_nlp

//game_nlp
Save:node\Pythons\project\Macro_Pythons_game_nlp.h




[4.2] 




[4.3] 




[4.4] 




[4.5] 




[4.6] 




[4.7] 




[4.8] 




[4.9] 




[4.10] 






[5.1] atec_2018_nlp
//Path:
//basePath = D:\project\NLP\game\atec_2018_nlp\

//
README.md
//code:
model.py
//run:
python model.py -m train_model -ms "ELMOEM128_CHAR"
//run--log:
Log_RUN.h run_log
//run--main:
Log_RUN.h fun_main













[5.2] Financial_NLP
//Path:
//basePath = D:\project\NLP\game\MLDemo\Financial_NLP\

//code:
final_demo/README.md

//管理整个项目的文件存放路径 
//存储和读取各种方法抽取的特征，并组合成DataFrame输出。
final_demo/util.py
//文本的清理工作 2.词向量训练工作
final_demo/data_prepare.py
//各种方法进行特征抽取，并进行保存
final_demo/extract_feature.py
//深度模型的构建工作
final_demo/train_model.py
//最后整个项目的运行，读取各个方法抽取的特征并使用分类模型进行分类预测
final_demo/main.py
python_w final_demo/main.py


[5.3] bert_google
//Path:
//basePath = D:\project\NLP\game\bert_tf_google
	
README.md
log\Log_file.h
//  log: 
log\Log_data.h
log\Log_preModel.h
log\Log_model.h
log\Log_train.h
log\Log_build.h
log\


[5.4] BERT_chinese
//	BERT 的全称是基于 Transformer 的双向编码器表征
//Path:
basePath = D:\project\NLP\game\BERT_chinese
	
README.md
log\Log_file.h
//  log: 
log\Log_data.h          Num -------------准备数据
log\Log_pretraining.h   Num -------------预训练
log\Log_fine_tuning.h   Num -------------微调
//log\Log_features.h      Num -------------抽取语义特征
//log\Log_classifier.h    Num -------------
//log\Log_squad.h         Num -------------
//  build: 
log\Log_build.h
log\Log_pretraining_base.h
log\tmp_pretraining_cur.h
log\
	








[5.5] 




[5.6] 




[5.7] 




[5.8] 




[5.9] 




[5.10] 




[6.1] 















